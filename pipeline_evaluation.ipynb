{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete evaluation pipeline with evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "nlp = spacy.load(\"en_core_web_trf\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"questions_answers.xlsx\")\n",
    "\n",
    "ocr_text = \"\"\" OCR TEXT HERE   \"\"\"\n",
    "\n",
    "cleaned_text = re.sub(r'\\s+', ' ', ocr_text)\n",
    "cleaned_text = re.sub(r'[^\\w\\s.]', '', cleaned_text)  \n",
    "cleaned_text = cleaned_text.strip()\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER using SpaCy\n",
    "doc = nlp(cleaned_text)\n",
    "entities = [ent.text for ent in doc.ents]\n",
    "ner_text = \" \".join(entities) \n",
    "print(\"\\nNER Extracted Text:\", ner_text)\n",
    "\n",
    "sentences = ner_text.split(\". \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = sentence_model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    return sentence_bleu(reference, candidate)\n",
    "\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    score = scorer.score(reference, candidate)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through questions and answers in the dataframe to evaluate and save it in an excel file.\n",
    "for idx, row in df.iterrows():\n",
    "    question = row['Question']\n",
    "    correct_answer = row['Original Answer']\n",
    "\n",
    "    question_embedding = sentence_model.encode(question, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(question_embedding, sentence_embeddings)\n",
    "\n",
    "    top_k = 3  \n",
    "    top_results = cosine_scores.topk(top_k)\n",
    "    relevant_sentences = [sentences[i] for i in top_results.indices[0]]\n",
    "\n",
    "    context = \" \".join(relevant_sentences)\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    generated_answer = result[\"answer\"]\n",
    "\n",
    "    bleu_score = calculate_bleu(correct_answer, generated_answer)\n",
    "    rouge_score = calculate_rouge(correct_answer, generated_answer)\n",
    "\n",
    "    results.append({\n",
    "        'Question': question,\n",
    "        'Generated Answer': generated_answer,\n",
    "        'BLEU Score': bleu_score,\n",
    "        'ROUGE Score': rouge_score['rouge1'].fmeasure,\n",
    "\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.to_csv(\"generated_answers_with_scores.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
